<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Here Comes the Sun</title><link href="https://sunng.info/blog/" rel="alternate"></link><link href="https://sunng.info/blog/blog/categories/clojure/atom.xml" rel="self"></link><id>https://sunng.info/blog/</id><updated>2015-10-18T13:45:37+08:00</updated><entry><title>"Summarizing Changes in Slacker 0.13"</title><link href="https://sunng.info/blog/summarizing-changes-in-slacker-013.html" rel="alternate"></link><updated>2015-10-18T13:45:37+08:00</updated><author><name>Ning Sun</name></author><id>tag:sunng.info,2015-10-18:blog/summarizing-changes-in-slacker-013.html</id><summary type="html">&lt;p&gt;After a year of feature development and minor fixes, &lt;a href="https://github.com/sunng87/slacker"&gt;Slacker&lt;/a&gt; and &lt;a href="https://github.com/sunng87/slacker-cluster"&gt;Slacker Cluster&lt;/a&gt; version 0.13 is now available. In this article, I will summarize changes in this release and give a you short introduction of new features and improvements.&lt;/p&gt;
&lt;p&gt;Slacker is an RPC framework features non-invasive design. It exposes clojure namespace as remote service, and keeps your remote invocation as simple as local version. Slacker cluster uses Zookeeper for service discovery, helps you to build micro-service based architecture. The grouping function gives you full control over request routing.&lt;/p&gt;
&lt;h3&gt;Application managed thread pool&lt;/h3&gt;
&lt;p&gt;During 0.12 series, Slacker server uses Netty managed thread pool for task execution. Netty assign a single thread from its pool to a connection. The thread will be used for all requests from the connection. And these requests will be processed in a serial manner. This works perfect for non-blocking tasks. However, if your tasks are data-intensive, this causes head-of-line blocking issue.&lt;/p&gt;
&lt;p&gt;The Netty design is to keep request/esponse ordered for a connection. Slacker uses multiplex on its connection, so ordering is not an issue. In 0.13, we now use an application managed thread pool for task execution. You can still configure the pool size by &lt;code&gt;:threads&lt;/code&gt; option. If your tasks are non-blocking ones, just set the threads equals your cores. Otherwise, you can customize the size based on blocking time of your tasks.&lt;/p&gt;
&lt;h3&gt;Interrupt&lt;/h3&gt;
&lt;p&gt;0.13 introduces a new low-level API called &lt;code&gt;interrupt&lt;/code&gt; and a new option &lt;code&gt;interrupt-on-timeout&lt;/code&gt;. This is backend by a new protocol level command, &lt;code&gt;interrupt&lt;/code&gt;. The new command allows the client to interrupt server execution for a particular task. The server thread will be released once &lt;code&gt;interrupt&lt;/code&gt; received.&lt;/p&gt;
&lt;p&gt;Typically you don't have to call &lt;code&gt;interrupt&lt;/code&gt; on slacker client. The &lt;code&gt;interrupt-on-timeout&lt;/code&gt; option allows you to cancel a tasks on both client and server when it's timeout. Following the design principle of transparency, the cancellation is synchronized to server-side, just like a local invocation.&lt;/p&gt;
&lt;h3&gt;Plug-able Serializers&lt;/h3&gt;
&lt;p&gt;To keep our dependency-tree clean, we detects cheshire/nippy/carbonite at runtime, and makes these dependencies totally optional to slacker.&lt;/p&gt;
&lt;p&gt;The new default serializer is Clojure EDN because it requires no additional packages. Slacker provides built-in support for cheshire(&lt;code&gt;:json&lt;/code&gt;) and nippy(&lt;code&gt;:nippy&lt;/code&gt;). &lt;a href="https://github.com/ptaoussanis/nippy"&gt;Nippy&lt;/a&gt; is high recommended for Slacker. It's a clojure-native binary format, compact and fast. We have been using nippy with Slacker in our production for a long time without any issue.&lt;/p&gt;
&lt;p&gt;You can also extend our serializer system by create new implementations for serializer multi-method.&lt;/p&gt;
&lt;h3&gt;Server data for Slacker Cluster&lt;/h3&gt;
&lt;p&gt;The new Slacker Cluster &lt;code&gt;start-slacker-server&lt;/code&gt; offers a new option &lt;code&gt;:server-data&lt;/code&gt;. It allows you to assign some data for this server, for example, the environment (production or stage?). The data will be stored to Zookeeper and synchronized to client side.&lt;/p&gt;
&lt;p&gt;In the client grouping function, you can use the data to filter servers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt; &lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;slacker.client.cluster&lt;/span&gt; &lt;span class="ss"&gt;:refer&lt;/span&gt; &lt;span class="ss"&gt;:all&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;def &lt;/span&gt;&lt;span class="nv"&gt;sc&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;clustered-slackerc&lt;/span&gt; &lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;defn-remote&lt;/span&gt; &lt;span class="nv"&gt;sc&lt;/span&gt; &lt;span class="nv"&gt;some-function&lt;/span&gt;
  &lt;span class="ss"&gt;:grouping&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;fn &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kd"&gt;ns &lt;/span&gt;&lt;span class="nv"&gt;func&lt;/span&gt; &lt;span class="nv"&gt;params&lt;/span&gt; &lt;span class="nv"&gt;servers&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
               &lt;span class="c1"&gt;;; test :prod? property of server data&lt;/span&gt;
               &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;rand-nth&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;filter &lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:prod?&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;server-data&lt;/span&gt; &lt;span class="o"&gt;@&lt;/span&gt;&lt;span class="nv"&gt;sc&lt;/span&gt; &lt;span class="nv"&gt;%&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="nv"&gt;servers&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The grouping function in this snippet filters production servers, and choose one from them to call.&lt;/p&gt;
&lt;p&gt;The server side looks pretty simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt; &lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;slacker.server.cluster&lt;/span&gt; &lt;span class="ss"&gt;:refer&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;start-slacker-server&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;start-slacker-server&lt;/span&gt; &lt;span class="nv"&gt;some-port&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;some-ns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="ss"&gt;:server-data&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:prod?&lt;/span&gt; &lt;span class="nv"&gt;true&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Besides of these features, we also fixed issues Zookeeper timeout issue on startup, ephemeral node lost and etc.&lt;/p&gt;
&lt;p&gt;After almost 4 years of development, we are stepping near to a 1.0 release. Hopefully we will reach the 1.0 milestone in 2016.&lt;/p&gt;</summary><category term="'programming'"></category><category term="'clojure'"></category></entry><entry><title>Running Ring web application on HTTP2 with rj9a</title><link href="https://sunng.info/blog/running-ring-web-application-on-http2-with-rj9a.html" rel="alternate"></link><updated>2015-07-25T14:54:54+08:00</updated><author><name>Ning Sun</name></author><id>tag:sunng.info,2015-07-25:blog/running-ring-web-application-on-http2-with-rj9a.html</id><summary type="html">&lt;p&gt;&lt;a href="https://github.com/sunng87/ring-jetty9-adapter"&gt;Ring-jetty9-adapter(rj9a)&lt;/a&gt; just received an update, the &lt;a href="https://clojars.org/info.sunng/ring-jetty9-adapter"&gt;0.9&lt;/a&gt;, with Jetty 9.3 adoption. The most important feature in this release is support for HTTP2. That means, you can run your Ring application on the new HTTP2 protocol.&lt;/p&gt;
&lt;p&gt;In case you still have no idea about HTTP2, it's the biggest update to HTTP, the protocol we use everyday and everywhere. In short, &lt;a href="https://en.wikipedia.org/wiki/HTTP/2"&gt;HTTP2&lt;/a&gt; introduces connection multiplex to reuse connection for several request/response simultaneously. Also the persisted connection makes server push possible, and that's part of HTTP2. HTTP2 uses TLS by default. In order to keep most servers backward compatible, we will run HTTP2 and HTTP1.1 on the same server and port. Modern client will detect server configuration on SSL handshake, via a TLS extension called ALPN. The server will list supported application layer protocols in SERVER HELLO and let client to choose what it understands.&lt;/p&gt;
&lt;p&gt;The basic part of HTTP2 is fully compatible for 1.1, so you won't have to modify your application code to use it. In rj9a, just add option &lt;code&gt;:h2? true&lt;/code&gt; to enable HTTP2. And &lt;code&gt;:h2c? true&lt;/code&gt; to enable its variance on plain socket.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;dummy-app&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;req&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:body&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;It works&amp;quot;&lt;/span&gt; &lt;span class="ss"&gt;:status&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;jetty/run-jetty&lt;/span&gt; &lt;span class="nv"&gt;dummy-app&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="ss"&gt;:port&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
                            &lt;span class="ss"&gt;:h2c?&lt;/span&gt; &lt;span class="nv"&gt;true&lt;/span&gt;
                            &lt;span class="ss"&gt;:h2?&lt;/span&gt; &lt;span class="nv"&gt;true&lt;/span&gt;
                            &lt;span class="ss"&gt;:ssl?&lt;/span&gt; &lt;span class="nv"&gt;true&lt;/span&gt;
                            &lt;span class="ss"&gt;:ssl-port&lt;/span&gt; &lt;span class="mi"&gt;5443&lt;/span&gt;
                            &lt;span class="ss"&gt;:keystore&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;...&amp;quot;&lt;/span&gt;
                            &lt;span class="ss"&gt;:key-password&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To test HTTP2 interface, you will need to install &lt;a href="https://nghttp2.org"&gt;nghttp&lt;/a&gt;. It's pretty similar to curl:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ nghttp -v https://localhost:5443
&lt;span class="o"&gt;[&lt;/span&gt;  0.000&lt;span class="o"&gt;]&lt;/span&gt; Connected
The negotiated protocol: h2-14
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send SETTINGS frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;12, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;niv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;2&lt;span class="o"&gt;)&lt;/span&gt;
         &lt;span class="o"&gt;[&lt;/span&gt;SETTINGS_MAX_CONCURRENT_STREAMS&lt;span class="o"&gt;(&lt;/span&gt;0x03&lt;span class="o"&gt;)&lt;/span&gt;:100&lt;span class="o"&gt;]&lt;/span&gt;
         &lt;span class="o"&gt;[&lt;/span&gt;SETTINGS_INITIAL_WINDOW_SIZE&lt;span class="o"&gt;(&lt;/span&gt;0x04&lt;span class="o"&gt;)&lt;/span&gt;:65535&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send PRIORITY frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;5, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;3&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;dep_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;201, &lt;span class="nv"&gt;exclusive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send PRIORITY frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;5, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;5&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;dep_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;101, &lt;span class="nv"&gt;exclusive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send PRIORITY frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;5, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;7&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;dep_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1, &lt;span class="nv"&gt;exclusive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send PRIORITY frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;5, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;9&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;dep_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;7, &lt;span class="nv"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1, &lt;span class="nv"&gt;exclusive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send PRIORITY frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;5, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;11&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;dep_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;3, &lt;span class="nv"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1, &lt;span class="nv"&gt;exclusive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.031&lt;span class="o"&gt;]&lt;/span&gt; send HEADERS frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;37, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x25, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;13&amp;gt;
         &lt;span class="p"&gt;;&lt;/span&gt; END_STREAM &lt;span class="p"&gt;|&lt;/span&gt; END_HEADERS &lt;span class="p"&gt;|&lt;/span&gt; PRIORITY
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;padlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;dep_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;11, &lt;span class="nv"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;16, &lt;span class="nv"&gt;exclusive&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
         &lt;span class="p"&gt;;&lt;/span&gt; Open new stream
         :method: GET
         :path: /
         :scheme: https
         :authority: localhost:5443
         accept: */*
         accept-encoding: gzip, deflate
         user-agent: nghttp2/1.0.1
&lt;span class="o"&gt;[&lt;/span&gt;  0.032&lt;span class="o"&gt;]&lt;/span&gt; recv SETTINGS frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;12, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;niv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;2&lt;span class="o"&gt;)&lt;/span&gt;
         &lt;span class="o"&gt;[&lt;/span&gt;SETTINGS_HEADER_TABLE_SIZE&lt;span class="o"&gt;(&lt;/span&gt;0x01&lt;span class="o"&gt;)&lt;/span&gt;:4096&lt;span class="o"&gt;]&lt;/span&gt;
         &lt;span class="o"&gt;[&lt;/span&gt;SETTINGS_INITIAL_WINDOW_SIZE&lt;span class="o"&gt;(&lt;/span&gt;0x04&lt;span class="o"&gt;)&lt;/span&gt;:65535&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.032&lt;span class="o"&gt;]&lt;/span&gt; send SETTINGS frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x01, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&amp;gt;
         &lt;span class="p"&gt;;&lt;/span&gt; ACK
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;niv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.032&lt;span class="o"&gt;]&lt;/span&gt; recv SETTINGS frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x01, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&amp;gt;
         &lt;span class="p"&gt;;&lt;/span&gt; ACK
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;niv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.033&lt;span class="o"&gt;]&lt;/span&gt; recv &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;13&lt;span class="o"&gt;)&lt;/span&gt; :status: 200
&lt;span class="o"&gt;[&lt;/span&gt;  0.033&lt;span class="o"&gt;]&lt;/span&gt; recv &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;13&lt;span class="o"&gt;)&lt;/span&gt; server: Jetty&lt;span class="o"&gt;(&lt;/span&gt;9.3.1.v20150714&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;  0.033&lt;span class="o"&gt;]&lt;/span&gt; recv HEADERS frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;20, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x04, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;13&amp;gt;
         &lt;span class="p"&gt;;&lt;/span&gt; END_HEADERS
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;padlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&lt;span class="o"&gt;)&lt;/span&gt;
         &lt;span class="p"&gt;;&lt;/span&gt; First response header
It works&lt;span class="o"&gt;[&lt;/span&gt;  0.033&lt;span class="o"&gt;]&lt;/span&gt; recv DATA frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;8, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x01, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;13&amp;gt;
         &lt;span class="p"&gt;;&lt;/span&gt; END_STREAM
&lt;span class="o"&gt;[&lt;/span&gt;  0.033&lt;span class="o"&gt;]&lt;/span&gt; send GOAWAY frame &amp;lt;&lt;span class="nv"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;8, &lt;span class="nv"&gt;flags&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0x00, &lt;span class="nv"&gt;stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0&amp;gt;
         &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;last_stream_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;0, &lt;span class="nv"&gt;error_code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;NO_ERROR&lt;span class="o"&gt;(&lt;/span&gt;0x00&lt;span class="o"&gt;)&lt;/span&gt;, opaque_data&lt;span class="o"&gt;(&lt;/span&gt;0&lt;span class="o"&gt;)=[])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The verbose output shows us every detail about request and response in HTTP2.&lt;/p&gt;
&lt;p&gt;Note that in order to run HTTP2, you will need JDK 8 / OpenJDK 1.8 and &lt;a href="http://www.eclipse.org/jetty/documentation/current/alpn-chapter.html#alpn-starting"&gt;put alpn-boot jar in your bootclasspath&lt;/a&gt;.  I have created &lt;a href="https://github.com/sunng87/lein-bootclasspath-deps"&gt;a leiningen plugin&lt;/a&gt; to manage bootclasspath in clojure project.&lt;/p&gt;
&lt;p&gt;The complete example is available in &lt;a href="https://github.com/sunng87/ring-jetty9-adapter/blob/master/examples/rj9a/http2.clj"&gt;github repository&lt;/a&gt;.&lt;/p&gt;</summary><category term="'programming'"></category><category term="'clojure'"></category></entry><entry><title>"Clojure Microservice architecture with Slacker Cluster"</title><link href="https://sunng.info/blog/clojure-microservice-architecture-with-slacker-cluster.html" rel="alternate"></link><updated>2014-07-08T22:12:52+08:00</updated><author><name>Ning Sun</name></author><id>tag:sunng.info,2014-07-08:blog/clojure-microservice-architecture-with-slacker-cluster.html</id><summary type="html">&lt;p&gt;&lt;a href="http://www.infoq.com/presentations/Micro-Services"&gt;Microservice&lt;/a&gt; has been a hot new concept in these days. Martin Fowler explained microservice &lt;a href="http://martinfowler.com/articles/microservices.html"&gt;in this article&lt;/a&gt;. From me, microservice is a set of fine-grained function units running on independent process, each of them are connected with light-weighted transports: RESTful API or light messaging queue.&lt;/p&gt;
&lt;p&gt;It's a new concept in enterprise architecture, since the last movement in the field promotes SOA architecture. SOA encourages architects to componentize their business logic in service, and deploy service bus(ESB) for integration. Microservice can be more concrete and light-weighted. The service units in Microservice can be any standalone function, or just a tier in traditional tier based development. These units can be deployed on dedicate process or grouped into a process.&lt;/p&gt;
&lt;p&gt;In clojure development at &lt;a href="https://avoscloud.com"&gt;avoscloud&lt;/a&gt;, we are using the &lt;a href="https://github.com/sunng87/slacker-cluster"&gt;slacker cluster framework&lt;/a&gt; for our microsrvice architecture.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/sunng87/slacker"&gt;Slacker RPC&lt;/a&gt; exposes services as  clojure namespace (pretty light-weighted) All functions in the namespace can be called from remote. A slacker server can expose any number of namespaces:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;start-slacker-server&lt;/span&gt; &lt;span class="mi"&gt;2014&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;my.serviceA&lt;/span&gt; &lt;span class="nv"&gt;my.serviceB&lt;/span&gt; &lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Slacker uses a binary protocol on TCP and configurable serialization (json/edn/&lt;a href="https://github.com/ptaoussanis/nippy"&gt;nippy&lt;/a&gt;) for communication, which is fast and compact.&lt;/p&gt;
&lt;p&gt;And in slacker cluster, exposed namespaces are registered on zookeeper as ephemeral nodes. The client doesn't have to know which service is deployed on which process. Instead, it connects to zookeeper and look up all process address for service it interests in. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;def &lt;/span&gt;&lt;span class="nv"&gt;sc&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;clustered-slacker-client&lt;/span&gt; &lt;span class="nv"&gt;zk-addr&lt;/span&gt; &lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;defn-remote&lt;/span&gt; &lt;span class="ss"&gt;&amp;#39;sc&lt;/span&gt; &lt;span class="nv"&gt;my.serviceA/fn-abc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;;;when calling remote function for the first time, the client will look up zookeeper for remote processes and cache the results&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;fn-abc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If there are more than one process available, the client library will balance the load on each process. And for stateful service, slacker cluster also elects master node to ensure all requests go to single process. (&lt;a href="http://sunng.info/blog/blog/2014/06/09/grouping-in-slacker-0-dot-12/"&gt;Slacker cluster grouping&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Zookeeper directory structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls /slacker/example-cluster/namespaces/
[my.serviceA, my.serviceB]

ls /slacker/example-cluster/namespaces/my.serviceA
[192.168.1.100:2104, 192.168.1.101:2014...]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Decoupling processes and services made microservice deployment quite flexible. Functional namespaces can be deployed on any process, standalone or grouped together, like Martin Fowler's chart &lt;a href="http://martinfowler.com/articles/microservices/images/sketch.png"&gt;shows&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All these nodes are also watched by clients. If a process crashed or put offline, the clients will get notified by zookeeper, then no requests will be made on that process. Also, when you exhausted service capacity, just simply put on another process, the client will soon balance load to the new node. Scaling services is easy like that.&lt;/p&gt;
&lt;p&gt;Thanks to zookeeper's watch mechanism, there's no need to configure service static and update while you add/remove nodes. This is especially important in large-scale deployment. (Since microservices are often find-grained, you will always have a lot of process to update/restart.)&lt;/p&gt;
&lt;p&gt;For more about Slacker Cluster, &lt;a href="https://github.com/sunng87/slacker-cluster"&gt;check my code repository&lt;/a&gt;.&lt;/p&gt;</summary><category term="programming"></category><category term="clojure"></category></entry><entry><title>"Slacker Cluster 0.12: Grouping"</title><link href="https://sunng.info/blog/slacker-cluster-012-grouping.html" rel="alternate"></link><updated>2014-06-09T21:06:39+08:00</updated><author><name>Ning Sun</name></author><id>tag:sunng.info,2014-06-09:blog/slacker-cluster-012-grouping.html</id><summary type="html">&lt;h2&gt;What are Slacker and Slacker Cluster&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/sunng87/slacker"&gt;Slacker&lt;/a&gt; is my side project started in late 2011. The goal of Slacker project is to provide a high performance RPC system for clojure, with elegant API. Slacker doesn't ruin your code. Your remote invocation looks exactly same as local, from code. That means you can switch back and forth at same time.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/sunng87/slacker-cluster"&gt;Slacker Cluster&lt;/a&gt; is a support module for running Slacker servers with multiple instances. Cluster enabled slacker server will publish all its served namespaces to Zookeeper cluster. The Cluster enabled client reads and watches these meta data. The most important feature of Slacker Cluster is you can add or remove servers without changing client configuration.&lt;/p&gt;
&lt;h2&gt;Grouping in Slacker Cluster&lt;/h2&gt;
&lt;p&gt;Started in 0.11, then enhanced in 0.12, Slacker Cluster now has flexible &lt;strong&gt;grouping&lt;/strong&gt; choices for your scenario. In Slacker Cluster, &lt;strong&gt;grouping&lt;/strong&gt; means which server(s) to call on a particular invocation.&lt;/p&gt;
&lt;p&gt;There and four kinds of grouping for you: &lt;code&gt;:random&lt;/code&gt;, &lt;code&gt;:leader&lt;/code&gt;, &lt;code&gt;:all&lt;/code&gt; and custom.&lt;/p&gt;
&lt;h3&gt;:random&lt;/h3&gt;
&lt;p&gt;By default, Slacker cluster clients use &lt;code&gt;:random&lt;/code&gt; grouping: select a random server from server list. Random grouping works great for stateless services. It automatically balances load of each server.&lt;/p&gt;
&lt;h3&gt;:leader&lt;/h3&gt;
&lt;p&gt;Slacker servers selects leader for each namespace they expose. So at any time there will be one and only one leader node for every namespaces. The &lt;code&gt;:leader&lt;/code&gt; grouping routes all invocations onto the leader node. This is required when your server has state, and you have to ensure the consistency and availability.&lt;/p&gt;
&lt;h3&gt;:all&lt;/h3&gt;
&lt;p&gt;As the name suggests, &lt;code&gt;:all&lt;/code&gt; grouping routes invocations on every node at same time. In other words, it's broadcast. Note that this grouping might change your function return values. In &lt;code&gt;:random&lt;/code&gt; and &lt;code&gt;:leader&lt;/code&gt; mode, there's only one server called, just like local invocation. In &lt;code&gt;:all&lt;/code&gt;, there's chances several servers are called and several values returned. I will talk about how to deal with these return values later.&lt;/p&gt;
&lt;h3&gt;Custom&lt;/h3&gt;
&lt;p&gt;You can also provide a function for dynamic grouping. For requested namespace, function and arguments, you can specify any server(s) or grouping option.&lt;/p&gt;
&lt;h2&gt;Grouping results&lt;/h2&gt;
&lt;p&gt;Grouping may break original behavior of you code by returning multiple values from multiple servers. But you still have full control over it. There are four types of value you can specify for results aggregation: &lt;code&gt;:single&lt;/code&gt;, &lt;code&gt;:vector&lt;/code&gt;, &lt;code&gt;:map&lt;/code&gt; and custom function.&lt;/p&gt;
&lt;p&gt;In short words:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;:single&lt;/code&gt; returns the first valid result, and behavior same as calling single server or local invocation. This is the default value.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:vector&lt;/code&gt; returns  a vector of all results.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;:map&lt;/code&gt; returns a map of all results, indexed by server addresses.&lt;/li&gt;
&lt;li&gt;Custom aggregation function accepts the results and allows you to merge the values.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Grouping exceptions&lt;/h2&gt;
&lt;p&gt;What happens when remote function threw exceptions? The grouping exception option defines that. When set to &lt;code&gt;:all&lt;/code&gt;, the client will raise an error only if all remote nodes broken. Otherwise, the broken result will be ignored and only valid results will apply &lt;code&gt;grouping-results&lt;/code&gt; rules. The opposite option is &lt;code&gt;:any&lt;/code&gt;, which mean client will raise error when any of calls is broken.&lt;/p&gt;
&lt;h2&gt;Granularity&lt;/h2&gt;
&lt;p&gt;The grouping options can be set to client level:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;clustered-slackerc&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cluster-name&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;127.0.0.1:2181&amp;quot;&lt;/span&gt; &lt;span class="ss"&gt;:grouping&lt;/span&gt; &lt;span class="ss"&gt;:leader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or function level:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;defn-remote&lt;/span&gt; &lt;span class="nv"&gt;sc&lt;/span&gt; &lt;span class="nv"&gt;slacker.example.api/timestamp&lt;/span&gt;
  &lt;span class="ss"&gt;:grouping&lt;/span&gt; &lt;span class="ss"&gt;:all&lt;/span&gt;
  &lt;span class="ss"&gt;:grouping-results&lt;/span&gt; &lt;span class="ss"&gt;:single&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/sunng87/slacker-cluster"&gt;Slacker Cluster&lt;/a&gt; has been used in our Avos Cloud backend for service integration. Feel free to let me know if you have interests or questions with this library.&lt;/p&gt;</summary><category term="clojure"></category><category term="programming"></category></entry><entry><title>"Fork-Join in Papaline"</title><link href="https://sunng.info/blog/fork-join-in-papaline.html" rel="alternate"></link><updated>2014-05-27T21:47:45+08:00</updated><author><name>Ning Sun</name></author><id>tag:sunng.info,2014-05-27:blog/fork-join-in-papaline.html</id><summary type="html">&lt;p&gt;&lt;a href="http://github.com/sunng87/papaline"&gt;Papaline&lt;/a&gt; 0.3 introduced a new model "fork-join" for task execution. It allows you to split a task into smaller units, and execute them in parallel.&lt;/p&gt;
&lt;p&gt;Before that, a task is processed as a single unit from the first stage to the second, the third and the last. Within a stage, all computing is done in a single thread.&lt;/p&gt;
&lt;p&gt;&lt;img alt="linear execution" src="http://i.imgur.com/w6RlNZo.png" /&gt;&lt;/p&gt;
&lt;p&gt;This model has limitation that you are required to execute any of your stage in serial. If your task has a few split-able units, it's always better to run them in parallel. Here we have &lt;code&gt;(fork)&lt;/code&gt; command for the situation.&lt;/p&gt;
&lt;p&gt;For example, you are using the &lt;em&gt;fanout-on-write&lt;/em&gt; model to build an activity stream. Once a user posted a new status, you need to find all followers(stage 1) of that user and append the status to their timeline(stage 2).&lt;/p&gt;
&lt;p&gt;In previous version of papaline, these two stages are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;find-followers&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;id&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;followers&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;query-db-for-followers&lt;/span&gt; &lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;followers&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;fanout-to-user-timeline&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;user-ids&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;doseq &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;user-id&lt;/span&gt; &lt;span class="nv"&gt;user-ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;write-redis-list&lt;/span&gt; &lt;span class="nv"&gt;user-id&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the second task, the msg is appended to user's timeline one by one.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;(fork)&lt;/code&gt;, the &lt;code&gt;fanout-to-user-timeline&lt;/code&gt; can be executed in parallel.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;find-followers&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;id&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;followers&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;query-db-for-followers&lt;/span&gt; &lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;fork&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map &lt;/span&gt;&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;vector &lt;/span&gt;&lt;span class="nv"&gt;%&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;followers&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;fanout-to-user-timeline&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;user-ids&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;write-redis-list&lt;/span&gt; &lt;span class="nv"&gt;user-id&lt;/span&gt; &lt;span class="nv"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After the &lt;code&gt;find-followers&lt;/code&gt; function, the result will be splitted into &lt;code&gt;(count followers)&lt;/code&gt; parts and sent into input channel of stage 2. So the tasks execution will be like:&lt;/p&gt;
&lt;p&gt;&lt;img alt="forked execution" src="http://i.imgur.com/MLhZ0Pm.png" /&gt;&lt;/p&gt;
&lt;p&gt;To collect the results of all forked sub-tasks, you can use &lt;code&gt;(join)&lt;/code&gt;. If the return value is wrapped with join, it won't trigger next stage immediately but to wait all forked tasks to finish.&lt;/p&gt;
&lt;p&gt;&lt;img alt="join" src="http://i.imgur.com/BVDEH9Q.png" /&gt;&lt;/p&gt;
&lt;p&gt;So with &lt;code&gt;(fork)&lt;/code&gt; and &lt;code&gt;(join)&lt;/code&gt;, it's very flexible to change execution model in Papaline.  Internally, I use clojure's &lt;a href="http://clojure.org/metadata"&gt;metadata&lt;/a&gt; to add flags for the return value, without ruining the non-invasive design of &lt;a href="http://github.com/sunng87/papaline"&gt;Papaline&lt;/a&gt;.&lt;/p&gt;</summary><category term="clojure"></category><category term="programming"></category></entry></feed>