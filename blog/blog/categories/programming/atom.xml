<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming | Here comes the Sun]]></title>
  <link href="http://sunng87.github.io/blog//blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://sunng87.github.io/blog//"/>
  <updated>2014-07-08T23:32:35+08:00</updated>
  <id>http://sunng87.github.io/blog//</id>
  <author>
    <name><![CDATA[Sun Ning]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Clojure Microservice Architecture With Slacker Cluster]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/07/08/microservice-and-slacker-cluster/"/>
    <updated>2014-07-08T22:12:52+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/07/08/microservice-and-slacker-cluster</id>
    <content type="html"><![CDATA[<p><a href="http://www.infoq.com/presentations/Micro-Services">Microservice</a> has been a hot new concept in these days. Martin Fowler explained microservice <a href="http://martinfowler.com/articles/microservices.html">in this article</a>. From me, microservice is a set of fine-grained function units running on independent process, each of them are connected with light-weighted transports: RESTful API or light messaging queue.</p>

<p>It&rsquo;s a new concept in enterprise architecture, since the last movement in the field promotes SOA architecture. SOA encourages architects to componentize their business logic in service, and deploy service bus(ESB) for integration. Microservice can be more concrete and light-weighted. The service units in Microservice can be any standalone function, or just a tier in traditional tier based development. These units can be deployed on dedicate process or grouped into a process.</p>

<p>In clojure development at <a href="https://avoscloud.com">avoscloud</a>, we are using the <a href="https://github.com/sunng87/slacker-cluster">slacker cluster framework</a> for our microsrvice architecture.</p>

<p><a href="https://github.com/sunng87/slacker">Slacker RPC</a> exposes services as  clojure namespace (pretty light-weighted) All functions in the namespace can be called from remote. A slacker server can expose any number of namespaces:</p>

<p><code>clojure
(start-slacker-server 2014 [my.serviceA my.serviceB ...])
</code></p>

<p>Slacker uses a binary protocol on TCP and configurable serialization (json/edn/<a href="https://github.com/ptaoussanis/nippy">nippy</a>) for communication, which is fast and compact.</p>

<p>And in slacker cluster, exposed namespaces are registered on zookeeper as ephemeral nodes. The client doesn&rsquo;t have to know which service is deployed on which process. Instead, it connects to zookeeper and look up all process address for service it interests in.</p>

<p>```clojure
(def sc (clustered-slacker-client zk-addr &hellip;))
(defn-remote &lsquo;sc my.serviceA/fn-abc)</p>

<p>;;when calling remote function for the first time, the client will look up zookeeper for remote processes and cache the results
(fn-abc)
```</p>

<p>If there are more than one process available, the client library will balance the load on each process. And for stateful service, slacker cluster also elects master node to ensure all requests go to single process. (<a href="http://sunng.info/blog/blog/2014/06/09/grouping-in-slacker-0-dot-12/">Slacker cluster grouping</a>)</p>

<p>Zookeeper directory structure:</p>

<p>```
ls /slacker/example-cluster/namespaces/
[my.serviceA, my.serviceB]</p>

<p>ls /slacker/example-cluster/namespaces/my.serviceA
[192.168.1.100:2104, 192.168.1.101:2014&hellip;]
```</p>

<p>Decoupling processes and services made microservice deployment quite flexible. Functional namespaces can be deployed on any process, standalone or grouped together, like Martin Fowler&rsquo;s chart <a href="http://martinfowler.com/articles/microservices/images/sketch.png">shows</a>.</p>

<p>All these nodes are also watched by clients. If a process crashed or put offline, the clients will get notified by zookeeper, then no requests will be made on that process. Also, when you exhausted service capacity, just simply put on another process, the client will soon balance load to the new node. Scaling services is easy like that.</p>

<p>Thanks to zookeeper&rsquo;s watch mechanism, there&rsquo;s no need to configure service static and update while you add/remove nodes. This is especially important in large-scale deployment. (Since microservices are often find-grained, you will always have a lot of process to update/restart.)</p>

<p>For more about Slacker Cluster, <a href="https://github.com/sunng87/slacker-cluster">check my code repository</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[扩展 Linux Ephemeral 端口限制]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/07/01/extend-linux-ephemeral-ports/"/>
    <updated>2014-07-01T17:11:58+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/07/01/extend-linux-ephemeral-ports</id>
    <content type="html"><![CDATA[<p>默认情况下，单一Linux能发起的客户端连接数是十分有限的，为此，我们要测试大规模连接程序时不得不启动很多客户端机器模拟连接。下面介绍一些增加单台Linux发起连接数的方法。</p>

<h3>增加文件打开数</h3>

<p>第一步最为基础的，提高打开文件描述符的数量。默认的情况下，这个配置为1024，是不能满足我们的使用的。增加到999999个：</p>

<p><code>
$ sudo ulimit -n 999999
</code></p>

<p>持久化这个配置，可以在<code>/etc/security/</code>（或<code>/etc/security.d/</code>，取决于你的发行版）下建立文件，增加</p>

<p><code>
*       hard    nofile      999999
*       soft    nofile      999999
</code></p>

<p>这将对所有用户起效。</p>

<h3>增加客户端端口数</h3>

<p>当Linux发起客户端连接时，如果没有显式指定，会给客户端socket绑定一个 ephemeral 端口。这个端口的范围是从这个区间选取的：</p>

<p>```
 $ cat /proc/sys/net/ipv4/ip_local_port_range
32768   61000</p>

<p>```</p>

<p>如果这个区间的端口耗尽，socket就会产生<code>cannot assign requested address</code>的错误。要增加端口范围，我们需要把他设置得更大：</p>

<p><code>
$ sudo echo "1025 65535" &gt; /proc/sys/net/ipv4/ip_local_port_range
</code></p>

<p>这样，单台机器就可以发出六万多个连接。</p>

<h3>增加虚拟网卡</h3>

<p>对于内存大一点的客户端机器，六万多个连接远不是其性能极限。由于IP消息中，一条消息是由 <code>src_addr</code>, <code>src_port</code>, <code>dst_addr</code>, <code>dst_port</code> 四元组标识，所以要增加连接，我们需要更多IP。在Linux上，我们可以启动虚拟网卡绑定额外的IP。</p>

<p><code>
$ sudo ifconfig eth0:0 192.168.1.100
$ sudo ifconfig eth0:1 192.168.1.101
...
</code></p>

<p>要关闭这些虚拟网卡</p>

<p><code>
$ sudo ifconfig eth0:0 down
</code></p>

<h3>使用虚拟网卡连接</h3>

<p>拥有多个IP之后，客户端socket需要显示绑定这些IP才行，以python为例，在connect前调用：</p>

<p><code>python
sock.bind((local_addr, local_port))
</code></p>

<p>可以指定连接的源地址和端口。在普通的Linux编程里，当你指定<code>local_port</code>为<code>0</code>时，Linux会分配一个之前提到的 ephemeral 端口。但是当使用虚拟IP时，如果仍然指定0，系统并不会因为IP不同而重用端口号，达到六万多的限制后，仍然会抛出不能获得地址的异常。</p>

<p>实际上是可以获得的，这里需要用户显式地指定端口好。如果需要大规模的连接，那就一个一个绑定好了。</p>

<h3>启用time_wait reuse和recycle</h3>

<p>Linux的socket进入<code>time_wait</code>后需要有一定的时间回收，之后端口才能重新使用。这在大规模测试的时候就比较麻烦，为了免去等待，可以打开<code>tw_reuse</code>和<code>tw_recycle</code>这两个选项。</p>

<p><code>
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_recycle
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_reuse
</code></p>

<p>注意这两个选项都比较激进，最好仅在测试机上使用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slacker Cluster 0.12: Grouping]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/06/09/grouping-in-slacker-0-dot-12/"/>
    <updated>2014-06-09T21:06:39+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/06/09/grouping-in-slacker-0-dot-12</id>
    <content type="html"><![CDATA[<h2>What are Slacker and Slacker Cluster</h2>

<p><a href="https://github.com/sunng87/slacker">Slacker</a> is my side project started in late 2011. The goal of Slacker project is to provide a high performance RPC system for clojure, with elegant API. Slacker doesn&rsquo;t ruin your code. Your remote invocation looks exactly same as local, from code. That means you can switch back and forth at same time.</p>

<p><a href="https://github.com/sunng87/slacker-cluster">Slacker Cluster</a> is a support module for running Slacker servers with multiple instances. Cluster enabled slacker server will publish all its served namespaces to Zookeeper cluster. The Cluster enabled client reads and watches these meta data. The most important feature of Slacker Cluster is you can add or remove servers without changing client configuration.</p>

<h2>Grouping in Slacker Cluster</h2>

<p>Started in 0.11, then enhanced in 0.12, Slacker Cluster now has flexible <strong>grouping</strong> choices for your scenario. In Slacker Cluster, <strong>grouping</strong> means which server(s) to call on a particular invocation.</p>

<p>There and four kinds of grouping for you: <code>:random</code>, <code>:leader</code>, <code>:all</code> and custom.</p>

<h3>:random</h3>

<p>By default, Slacker cluster clients use <code>:random</code> grouping: select a random server from server list. Random grouping works great for stateless services. It automatically balances load of each server.</p>

<h3>:leader</h3>

<p>Slacker servers selects leader for each namespace they expose. So at any time there will be one and only one leader node for every namespaces. The <code>:leader</code> grouping routes all invocations onto the leader node. This is required when your server has state, and you have to ensure the consistency and availability.</p>

<h3>:all</h3>

<p>As the name suggests, <code>:all</code> grouping routes invocations on every node at same time. In other words, it&rsquo;s broadcast. Note that this grouping might change your function return values. In <code>:random</code> and <code>:leader</code> mode, there&rsquo;s only one server called, just like local invocation. In <code>:all</code>, there&rsquo;s chances several servers are called and several values returned. I will talk about how to deal with these return values later.</p>

<h3>Custom</h3>

<p>You can also provide a function for dynamic grouping. For requested namespace, function and arguments, you can specify any server(s) or grouping option.</p>

<h2>Grouping results</h2>

<p>Grouping may break original behavior of you code by returning multiple values from multiple servers. But you still have full control over it. There are four types of value you can specify for results aggregation: <code>:single</code>, <code>:vector</code>, <code>:map</code> and custom function.</p>

<p>In short words:</p>

<ul>
<li><code>:single</code> returns the first valid result, and behavior same as calling single server or local invocation. This is the default value.</li>
<li><code>:vector</code> returns  a vector of all results.</li>
<li><code>:map</code> returns a map of all results, indexed by server addresses.</li>
<li>Custom aggregation function accepts the results and allows you to merge the values.</li>
</ul>


<h2>Grouping exceptions</h2>

<p>What happens when remote function threw exceptions? The grouping exception option defines that. When set to <code>:all</code>, the client will raise an error only if all remote nodes broken. Otherwise, the broken result will be ignored and only valid results will apply <code>grouping-results</code> rules. The opposite option is <code>:any</code>, which mean client will raise error when any of calls is broken.</p>

<h2>Granularity</h2>

<p>The grouping options can be set to client level:</p>

<p><code>clojure
(clustered-slackerc "cluster-name" "127.0.0.1:2181" :grouping :leader)
</code></p>

<p>or function level:</p>

<p><code>clojure
(defn-remote sc slacker.example.api/timestamp
  :grouping :all
  :grouping-results :single)
</code></p>

<h2>Conclusion</h2>

<p><a href="https://github.com/sunng87/slacker-cluster">Slacker Cluster</a> has been used in our Avos Cloud backend for service integration. Feel free to let me know if you have interests or questions with this library.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fork-Join in Papaline]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/05/27/fork-join-in-papaline/"/>
    <updated>2014-05-27T21:47:45+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/05/27/fork-join-in-papaline</id>
    <content type="html"><![CDATA[<p><a href="http://github.com/sunng87/papaline">Papaline</a> 0.3 introduced a new model &ldquo;fork-join&rdquo; for task execution. It allows you to split a task into smaller units, and execute them in parallel.</p>

<p>Before that, a task is processed as a single unit from the first stage to the second, the third and the last. Within a stage, all computing is done in a single thread.</p>

<p><img src="http://i.imgur.com/w6RlNZo.png" alt="linear execution" /></p>

<p>This model has limitation that you are required to execute any of your stage in serial. If your task has a few split-able units, it&rsquo;s always better to run them in parallel. Here we have <code>(fork)</code> command for the situation.</p>

<p>For example, you are using the <em>fanout-on-write</em> model to build an activity stream. Once a user posted a new status, you need to find all followers(stage 1) of that user and append the status to their timeline(stage 2).</p>

<p>In previous version of papaline, these two stages are:</p>

<p>```clojure
(defn find-followers [id msg]
  (let [followers (query-db-for-followers id)]</p>

<pre><code>[followers msg]))
</code></pre>

<p>(defn fanout-to-user-timeline [user-ids msg]
  (doseq [user-id user-ids]</p>

<pre><code>(write-redis-list user-id msg)))
</code></pre>

<p>```</p>

<p>In the second task, the msg is appended to user&rsquo;s timeline one by one.</p>

<p>Using <code>(fork)</code>, the <code>fanout-to-user-timeline</code> can be executed in parallel.</p>

<p>```clojure
(defn find-followers [id msg]
  (let [followers (query-db-for-followers id)]</p>

<pre><code>(fork (map #(vector % msg) followers))))
</code></pre>

<p>(defn fanout-to-user-timeline [user-ids msg]
  (write-redis-list user-id msg))</p>

<p>```</p>

<p>After the <code>find-followers</code> function, the result will be splitted into <code>(count followers)</code> parts and sent into input channel of stage 2. So the tasks execution will be like:</p>

<p><img src="http://i.imgur.com/MLhZ0Pm.png" alt="forked execution" /></p>

<p>To collect the results of all forked sub-tasks, you can use <code>(join)</code>. If the return value is wrapped with join, it won&rsquo;t trigger next stage immediately but to wait all forked tasks to finish.</p>

<p><img src="http://i.imgur.com/BVDEH9Q.png" alt="join" /></p>

<p>So with <code>(fork)</code> and <code>(join)</code>, it&rsquo;s very flexible to change execution model in Papaline.  Internally, I use clojure&rsquo;s <a href="http://clojure.org/metadata">metadata</a> to add flags for the return value, without ruining the non-invasive design of <a href="http://github.com/sunng87/papaline">Papaline</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Papaline: Concurrent Pipeline With core.async]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/04/20/concurrent-pipeline-with-core-async/"/>
    <updated>2014-04-20T17:21:44+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/04/20/concurrent-pipeline-with-core-async</id>
    <content type="html"><![CDATA[<p>According to <a href="http://en.wikipedia.org/wiki/Staged_event-driven_architecture">wikipedia</a>, Staged Event-driven Architecture is an approach to software architecture that decomposes a complex, event-driven application into a set of stages connected by queues. We were using Java framework, <a href="https://github.com/sunng87/stages">stages</a>, to implement queue based SEDA. As we are using more and more Clojure nowadays, I decide to re-implement it in Clojure language, and in Clojure way. It&rsquo;s <a href="https://github.com/sunng87/papaline">papaline</a>.</p>

<p>The most important difference between papaline and stages is the usage of IoC threads. Core.async introduces IoC threads for Clojure, which is a popular concurrent mechanism recently. In traditional queue based thread pool, threads are blocked on queue to wait for tasks. While for IoC threads, channels act similar to queues but no actual thread is blocked on channel. Once there is a task available in channel, an underlying thread will be picked to execute it. So for core.async, you don&rsquo;t have to assign a static thread pool to each channel. The channel will pick thread from a shared system thread pool on demand. In current core.async release, it&rsquo;s a fixed thread pool with <em>(processors * 4) + 42</em> threads. That&rsquo;s much flexible and efficient.</p>

<p>Papaline takes advantages of this feature. The base concept in papaline is <strong>stage</strong> and <strong>pipeline</strong>. A pipeline is created with a ordered sequence of stages. Stages configured in a pipeline are connected with channels, instead of queues. Threads are automatically managed by core.async, and scheduled based on load of channels.</p>

<p>When you run a pipeline, the input data is sent to the inbound channel of the first stage. The stage will received the data and pick a thread to execute the function. Then the result is put into the second stage&rsquo;s inbound channel. The user-visible behavior is much like <code>comp</code>, but in concurrent.</p>

<p>Also core.async offers different type of channel buffers: fixed <code>buffer</code>, <code>sliding-buffer</code> and <code>dropping-buffer</code>. They are channels equivalent to j.u.c thread pool&rsquo;s <code>RejectedExecutionHandler</code>.</p>

<p>We have already deployed papaline in our asynchronous system and it works great by far. Find the project on <a href="https://github.com/sunng87/papaline">github</a> if you are interested in.</p>
]]></content>
  </entry>
  
</feed>
