<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming | Here comes the Sun]]></title>
  <link href="http://sunng87.github.io/blog//blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://sunng87.github.io/blog//"/>
  <updated>2015-01-25T17:08:28+08:00</updated>
  <id>http://sunng87.github.io/blog//</id>
  <author>
    <name><![CDATA[Sun Ning]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Handlebars 的 Rust 实现]]></title>
    <link href="http://sunng87.github.io/blog//blog/2015/01/25/ann-handlebars-rust/"/>
    <updated>2015-01-25T16:22:50+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2015/01/25/ann-handlebars-rust</id>
    <content type="html"><![CDATA[<p>本来一度感觉到用过 Clojure 之后很难对新语言产生兴趣了，还好遇到了 Rust 再次激活了这方面的生命力。今年的重点之一是学习 Rust 语言，方便自己能真正 touch bare metal。1月17号的 Rust 聚会上发现很多人都持有类似的想法。 C++ 之后鲜有这种语言，以至于之后成长起来的一代人都是在一个 VM 里编程，无论是 Java 还是 Python，最终都没有办法自己去管理内存，Rust 的出现给了大家一个机会。一个具备现代特性的系统编程语言，Zero runtime，可以运行在各种设备上。我也是其中一员，去年还给<a href="">程序员杂志写了一篇 Rust 的文章</a>，结果现在程序员杂志也停刊了。</p>

<p>扯远了，和当时学 Clojure 一样，这次的计划还是写一个正经的项目来促进学习。关于时机的选择，主要是 crates.io 仓库的发布基本上标志生态圈开始建立了，这个时候写东西就方便很多了。</p>

<p>这次选的就是实现 Handlebars，主要原因是 rust 已经逐渐有一点 web 开发的生态圈了，但是缺少一个模版引擎，于是我就来趟这潭浑水吧。为什么是 Handlebars 呢：</p>

<ul>
<li>不要把 rust 代码写进 html 模版里，反例： jsp, ejs</li>
<li>不要把 html 代码写进 rust 里，反例： hiccup</li>
<li>能够复用，基于“继承”而不是 include</li>
<li>能够简单地自定义标签，反例：mustach</li>
</ul>


<p>基于以上的原则，<a href="https://github.com/sunng87/handlebars-rust">handlebars-rust</a> 实现了基本的模版解析、渲染，重用机制（partial/include）和自定义 helper。除了不支持一些 mustach 风格的语法以外（可以用 #each / #if 这样的 helper 替代，更清晰），基本上所有的 handlebars 功能全部支持了。如果有遗漏的话欢迎 PR。另外还写了一个 <a href="https://github.com/sunng87/handlebars-iron">handlebars-iron</a> 项目，作为一个 <a href="http://ironframework.io">Iron 框架</a>的 middlaware。</p>

<p>简单总结几点收获：</p>

<ul>
<li>Rust 中要实现类似OO的多态需要用枚举类型，trait可以用来做范型</li>
<li>静态类型语言和一个基于 javascript 视角的模版引擎对接很困难，比如 js 里有 falsy 的概念，if 的判断里 <code>false</code>/<code>0</code>/<code>""</code>/<code>[]</code> 这些值都是 false，但是在rust里需要根据不同类型作判断，直接使用简直不可能。所以在 handlebars-rust 里利用了 rustc-serialize 里的 <code>Json</code> 枚举类型（没有真正序列化），要求所有渲染的数据都必须实现 <code>ToJson</code>，算是设计上的一个取舍。</li>
<li>Rust 的 derive 是一个神奇的功能，后来发现确实是一个 magic，因为可以 derive 的 trait 都是写死在编译器里的</li>
<li>关于 Rust 的 ownership 看<a href="http://nercury.github.io/rust/guide/2015/01/19/ownership.html">这篇文章</a>，作者承诺再写一篇关于 borrow 和 lifetime 的，相信也不错</li>
<li>有任何问题都可以在 stackoverflow 上问，有几个人会很快回复</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVOS Cloud 实时通信服务架构：微服务和服务发现]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/08/19/avoscloud-rtm-architecture-microservice-and-service-discovery/"/>
    <updated>2014-08-19T16:46:20+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/08/19/avoscloud-rtm-architecture-microservice-and-service-discovery</id>
    <content type="html"><![CDATA[<p>“微服务” (Microservice) 是今年特别热的一个概念，<a href="http://martinfowler.com/articles/microservices.html">Martin Fowler 的文章</a>对微服务作了详细的介绍。简而言之，微服务鼓励用户把功能拆分以细粒度的服务接口暴露出来，并通过REST 服务或轻量级消息队列集成。在微服务架构里，一个业务的实现，可能由不同的功能单元组合而实现。</p>

<p>在 AVOS Cloud，我们提供数据存储、统计、实时通信等不同功能的服务，在实现上，这些功能需要共用基础设施，有的服务本身也根据业务性质的不同拆分功能模块，我们目前就是以这种“微服务”架构思路来实现拆分。有句话说，if you cannot split, you cannot scale.</p>

<p>以实时通信服务为例，根据功能角色的不同，我们有这样一些模块：</p>

<ul>
<li>Push 服务：处理推送的订阅关系，触发推送</li>
<li>长连接服务器：维持设备与服务的长连接</li>
<li>Router：应用层的 lookup，负责分配合适的长连接服务器给新设备</li>
<li>WatchDog：从多台长连接服务器收集运行和统计数据，对异常情况发起报警</li>
<li>数据存储：群组数据，用户可以通过API访问</li>
<li>HBase：消息记录存储</li>
</ul>


<p>模块间的集成，根据业务的特性分别使用 <a href="https://github.com/sunng87/slacker">Slacker</a> 远程调用框架和 <a href="http://twitter.github.io/kestrel/">Kestrel</a> 消息队列。
对于可能耗时较大的任务，我们通过 Kestrel 队列放到后台执行，避免阻塞前台服务，影响吞吐量。而另一些需要实时的集成，则使用 Slacker 远程调用实现多个进程间的通信。</p>

<p>Slacker 是一个专门为 Clojure 语言设计的远程调用框架，利用 Slacker 你可以暴露一个 或多个 Clojure 的 namespace 供远程调用。在客户端，Slacker 利用 <a href="http://clojure.org/macros">Clojure 宏</a>的特性，保持远程调用和本地调用的代码完全一致，这样本地和远程调用的切换只要更改一个 <code>(require)</code> 即可实现，把框架对业务代码的侵入降到最低。此外，Slacker 使用二进制序列化 <a href="https://github.com/ptaoussanis/nippy">nippy</a>，在网络连接层面使用异步复用，同时在超时方面也做了良好的控制。</p>

<p>以上的基础设施帮助我们良好地拆分模块，为下一步的扩展提供了可能。</p>

<h3>服务发现</h3>

<p>长连接服务器是实时通信的功能核心，它的瓶颈在内存和 CPU，可以通过增加部署来达到线性扩展。随着业务量的增加和硬件资源的整合，它可能会面临较频繁的部署变化，另外它也需要有能通过新增部署来快速平滑高峰压力的能力。基于 <a href="https://en.wikipedia.org/wiki/Publish/subscribe">Pub/Sub 抽象</a>的消息队列对此有良好的支持，但这对我们以 RPC 为核心的集成方式提出了新的要求，依赖模块也能快速响应服务部署的变化：我们不可能在新增某个服务部署后修改每个依赖的配置再逐一重启。</p>

<p>在这方面，我们利用了 <a href="https://github.com/sunng87/slacker-cluster">Slacker Cluster 框架</a>。他的核心思想是在部署和服务间增加一层抽象：对于服务的消费者而言，只需声明自己所依赖的服务，而无需静态地了解进程的地址。</p>

<p>所有的服务提供者将自己能够提供的服务注册在 <a href="https://zookeeper.apache.org/">Zookeeper</a> 集群里，并将部署地址注册为 Ephemeral 节点。Ephemeral 节点在创建它的连接断后会自动删除，这样当一个部署下线后，它相应的节点也会自动删除。</p>

<p>```bash</p>

<h1>Zookeeper 目录结构</h1>

<p>ls /slacker/example-cluster/namespaces/
[my.serviceA, my.serviceB]</p>

<p>ls /slacker/example-cluster/namespaces/my.serviceA
[192.168.1.100:2104, 192.168.1.101:2014&hellip;]
```</p>

<p>所有服务的客户端会 watch 自己感兴趣的 Zookeeper 节点，而部署变化时，所有的客户端都会得到通知，进而刷新服务列表，将流量引向新的节点。</p>

<p>在实时通信服务中，Router 服务会通过这个 RPC 机制轮询所有在线的长连接服务器，记录他们实时的运行状态。所有的用户设备并非直接连接到固定的长连接服务器，而是先询问 Router，由后者分配一台压力较轻的实例。当有新的长连接服务器部署后，Router收到通知，新的连接将优先连接这个新进程。此外，监控和数据收集的服务也会自动地把新实例加入管理范围。</p>

<p>有了这样一套服务发现机制，我们就可以对整个架构中的任意模块随时增减部署，保证服务可以以健康的状态运行。未来，我们还会集成云主机的提供商的API，来实现基础设施的自动化：当系统压力达到阀值时，云主机自动分配新的资源自动开机，jenkins 自动部署，加上现有的服务发现机制，实现0手工操作。这将是云服务运维的新篇。</p>

<p>原载 <a href="https://blog.avoscloud.com/1927/">AVOSCloud Blog</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在 Docker 中安装和使用 Rust Nightly 版本]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/08/02/rust-with-docker/"/>
    <updated>2014-08-02T18:02:12+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/08/02/rust-with-docker</id>
    <content type="html"><![CDATA[<p>一直关注 Rust 语言，最近一下发现了两个 web 框架，<a href="http://ironframework.org">Iron</a> 和 <a href="http://nickel.rs">Nickel.rs</a>。先不说这两个框架成熟度如何，一般情况下，一个语言有了 web 框架，算是一个里程碑，说明他离靠谱也不远了。这样我决定跟一下 nightly 版本（新框架都是跟 nightly），另外也能感受一下 <a href="https://mail.mozilla.org/pipermail/rust-dev/2014-March/009090.html">Yehuda Katz 的构建工具 Cargo</a>。ArchLinux 的仓库里已经有 0.11 版本，再用脚本安装必然会有冲突。于是想到了最近半年<a href="https://twitter.com/jessenoller/status/495037475421954048">最火的 Docker</a>，可以轻松的创建多个环境，正是一个非常好的场景。</p>

<h2>安装</h2>

<p>安装 Docker, Arch Linux 仓库里很早就有，非常方便：<code>sudo pacman -S docker</code>。完成之后启动他：<code>sudo systemctl start docker</code>。</p>

<p>之后我们拉一个 ubuntu 的镜像下来：<code>docker pull ubuntu</code>。</p>

<p>完成之后，我们启动一个 container，做一些基本的 setup：<code>docker run -i -t ubuntu:14.04 /bin/bash</code></p>

<p>这相当与运行在 ubuntu:14.04 这个镜像上运行一个 shell，接下来就进入了这个 shell 环境，和 ubuntu 安装版本完全一致，我们做一些基础的准备，安装一些必要的工具：<code>apt-get install build-essentials git curl libssl-dev</code></p>

<p>之后，就可以下载 Rust 提供的脚本来安装 nightly 版本了：<code>curl -s http://www.rust-lang.org/rustup.sh &gt; rustup</code></p>

<p>这里有个问题，rustup 脚本判断64位系统时会出错导致安装失败：
```sh</p>

<h1>Detect 64 bit linux systems with 32 bit userland and force 32 bit compilation</h1>

<p>if [ $CFG_OSTYPE = unknown-linux-gnu -a $CFG_CPUTYPE = x86_64 ]
then</p>

<pre><code>file -L "$SHELL" | grep -q "x86[_-]64"
if [ $? != 0 ]; then
    CFG_CPUTYPE=i686
fi
</code></pre>

<p>fi
```</p>

<p>因为在我的机器上已知系统是64位，就强行绕过了他的判断。</p>

<p>```bash
if [ $CFG_OSTYPE = unknown-linux-gnu -a $CFG_CPUTYPE = x86_64 ]
then</p>

<pre><code>file -L "$SHELL" | grep -q "x86[_-]64"
if [ $? == 0 ]; then
    CFG_CPUTYPE=i686
fi
</code></pre>

<p>fi
```</p>

<p>之后执行 rustup 就可以直接安装最近的 rustc 和 cargo 了。安装完成执行 <code>rustc -v</code> 和 <code>cargo --version</code> （两个工具还不统一！）可以了解安装情况。</p>

<p>exit 退出 shell，commit 你的镜像，这样一个干净的镜像要好好保存：<code>docker commit IMAGE_ID sunng/rust-nightly</code></p>

<h2>Hello World</h2>

<p>之后可以写点代码了，我们不在 docker 里写，我们在 host 机器上写，然后挂载到 docker 上，因此 emacs 什么的也不用配置了。</p>

<p>创建一个目录，比如在 <code>$HOME/var/docker/helloworld</code>下，最简单的 rust 项目只要两个文件： <code>Cargo.toml</code> 和 <code>src/main.rs</code>。</p>

<p>```</p>

<h1>Cargo.toml</h1>

<p>[package]</p>

<p>name = &ldquo;hello-world&rdquo;
version = &ldquo;0.1.0&rdquo;
authors = [ &ldquo;<a href="&#x6d;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#115;&#x75;&#110;&#x6e;&#103;&#x40;&#x61;&#x62;&#x6f;&#117;&#116;&#46;&#109;&#x65;">&#115;&#x75;&#110;&#x6e;&#103;&#64;&#x61;&#98;&#x6f;&#x75;&#116;&#x2e;&#x6d;&#101;</a>&rdquo; ]
```</p>

<p>```rust
//main.rs</p>

<p>fn main() {
  println!(&ldquo;hello world&rdquo;);
}</p>

<p>```</p>

<p>构建项目不需要手动 rustc 了，那是上个世纪的东西，我们直接 <code>cargo build</code> 就可以：<code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld sunng/nightly cargo build</code></p>

<p>其中 <code>-v</code> 参数用于挂载目录，<code>-w</code> 参数指定执行的 pwd。</p>

<p>如果构建成功，就可以执行了，在 docker 中执行：<code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld sunng/nightly target/hello-world</code></p>

<p>其实可以直接在 host 系统里执行也是完全可以的：<code>$HOME/var/docker/helloworld/target/hello-world</code>。</p>

<h2>Web Hello World</h2>

<p>前面说了 Rust 都有 web 框架了，我们就写一个 Web 版本的 Hello World 吧。这次用 Iron 框架，首先添加依赖到 Cargo 文件：</p>

<p>```
[package]</p>

<p>name = &ldquo;hello-world&rdquo;
version = &ldquo;0.1.0&rdquo;
authors = [ &ldquo;<a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#111;&#x3a;&#115;&#x75;&#110;&#x6e;&#x67;&#64;&#x61;&#x62;&#111;&#x75;&#x74;&#46;&#x6d;&#101;">&#115;&#x75;&#x6e;&#110;&#103;&#x40;&#97;&#98;&#111;&#x75;&#x74;&#46;&#x6d;&#101;</a>&rdquo; ]</p>

<p>[dependencies.iron]</p>

<p>git = &ldquo;<a href="https://github.com/iron/iron.git">https://github.com/iron/iron.git</a>&rdquo;</p>

<p>[dependencies.core]</p>

<p>git = &ldquo;<a href="https://github.com/iron/core.git">https://github.com/iron/core.git</a>&rdquo;
```</p>

<p>Cargo 目前还没有中央仓库，但是<a href="http://crates.io/faq.html#github">据说将来会有</a>。目前还都是用 git 仓库来直接添加，所以构建环境里必须要有 git。</p>

<p>照着 <a href="https://github.com/iron/iron/blob/master/examples/hello.rs">Iron 的例子</a>写一个最简单的 hello world 程序。</p>

<p>```rust
extern crate iron;
extern crate http;</p>

<p>use std::io::net::ip::Ipv4Addr;
use iron::{Iron, Server, Chain, Request, Response, Alloy, Status, Unwind, FromFn};
use <a href="http::status;">http::status;</a></p>

<p>fn hello_world(<em>: &amp;mut Request, res: &amp;mut Response, </em>: &amp;mut Alloy) &ndash;> Status {</p>

<pre><code>res.serve(status::Ok, "Hello, world!");
Unwind
</code></pre>

<p>}</p>

<p>fn main() {
  let mut server: Server = Iron::new();
  server.chain.link(FromFn::new(hello_world));
  server.listen(Ipv4Addr(127, 0, 0, 1), 3000);
}</p>

<p>```</p>

<p>编译 <code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld sunng/nightly cargo build</code></p>

<p>运行 <code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld -p 3000:3000 sunng/nightly target/hello-world</code></p>

<p>新增的参数<code>-p</code>是把 docker 环境里的端口3000映射到 host 上的3000，这样我们才能在外面访问。</p>

<p>最后还有一个问题，因为程序听的是127.0.0.1，所以在 host 上是无法访问这个端口的，修改代码：</p>

<p>```rust</p>

<p>  server.listen(Ipv4Addr(0, 0, 0, 0), 3000);
```</p>

<p>就可以正常工作了。</p>

<h2>Wrap up</h2>

<p>总结一下上面用 docker 比虚拟机的好处：</p>

<ul>
<li>占用资源少，启动快</li>
<li>与 host 共享网络、硬盘都非常方便，满足开发需要不成问题</li>
<li>所有都是命令，与 host 系统上的进程集成也非常方便</li>
<li>支持镜像的版本控制和仓库</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clojure Microservice Architecture With Slacker Cluster]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/07/08/microservice-and-slacker-cluster/"/>
    <updated>2014-07-08T22:12:52+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/07/08/microservice-and-slacker-cluster</id>
    <content type="html"><![CDATA[<p><a href="http://www.infoq.com/presentations/Micro-Services">Microservice</a> has been a hot new concept in these days. Martin Fowler explained microservice <a href="http://martinfowler.com/articles/microservices.html">in this article</a>. From me, microservice is a set of fine-grained function units running on independent process, each of them are connected with light-weighted transports: RESTful API or light messaging queue.</p>

<p>It&rsquo;s a new concept in enterprise architecture, since the last movement in the field promotes SOA architecture. SOA encourages architects to componentize their business logic in service, and deploy service bus(ESB) for integration. Microservice can be more concrete and light-weighted. The service units in Microservice can be any standalone function, or just a tier in traditional tier based development. These units can be deployed on dedicate process or grouped into a process.</p>

<p>In clojure development at <a href="https://avoscloud.com">avoscloud</a>, we are using the <a href="https://github.com/sunng87/slacker-cluster">slacker cluster framework</a> for our microsrvice architecture.</p>

<p><a href="https://github.com/sunng87/slacker">Slacker RPC</a> exposes services as  clojure namespace (pretty light-weighted) All functions in the namespace can be called from remote. A slacker server can expose any number of namespaces:</p>

<p><code>clojure
(start-slacker-server 2014 [my.serviceA my.serviceB ...])
</code></p>

<p>Slacker uses a binary protocol on TCP and configurable serialization (json/edn/<a href="https://github.com/ptaoussanis/nippy">nippy</a>) for communication, which is fast and compact.</p>

<p>And in slacker cluster, exposed namespaces are registered on zookeeper as ephemeral nodes. The client doesn&rsquo;t have to know which service is deployed on which process. Instead, it connects to zookeeper and look up all process address for service it interests in.</p>

<p>```clojure
(def sc (clustered-slacker-client zk-addr &hellip;))
(defn-remote &lsquo;sc my.serviceA/fn-abc)</p>

<p>;;when calling remote function for the first time, the client will look up zookeeper for remote processes and cache the results
(fn-abc)
```</p>

<p>If there are more than one process available, the client library will balance the load on each process. And for stateful service, slacker cluster also elects master node to ensure all requests go to single process. (<a href="http://sunng.info/blog/blog/2014/06/09/grouping-in-slacker-0-dot-12/">Slacker cluster grouping</a>)</p>

<p>Zookeeper directory structure:</p>

<p>```
ls /slacker/example-cluster/namespaces/
[my.serviceA, my.serviceB]</p>

<p>ls /slacker/example-cluster/namespaces/my.serviceA
[192.168.1.100:2104, 192.168.1.101:2014&hellip;]
```</p>

<p>Decoupling processes and services made microservice deployment quite flexible. Functional namespaces can be deployed on any process, standalone or grouped together, like Martin Fowler&rsquo;s chart <a href="http://martinfowler.com/articles/microservices/images/sketch.png">shows</a>.</p>

<p>All these nodes are also watched by clients. If a process crashed or put offline, the clients will get notified by zookeeper, then no requests will be made on that process. Also, when you exhausted service capacity, just simply put on another process, the client will soon balance load to the new node. Scaling services is easy like that.</p>

<p>Thanks to zookeeper&rsquo;s watch mechanism, there&rsquo;s no need to configure service static and update while you add/remove nodes. This is especially important in large-scale deployment. (Since microservices are often find-grained, you will always have a lot of process to update/restart.)</p>

<p>For more about Slacker Cluster, <a href="https://github.com/sunng87/slacker-cluster">check my code repository</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[扩展 Linux Ephemeral 端口限制]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/07/01/extend-linux-ephemeral-ports/"/>
    <updated>2014-07-01T17:11:58+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/07/01/extend-linux-ephemeral-ports</id>
    <content type="html"><![CDATA[<p>默认情况下，单一Linux能发起的客户端连接数是十分有限的，为此，我们要测试大规模连接程序时不得不启动很多客户端机器模拟连接。下面介绍一些增加单台Linux发起连接数的方法。</p>

<h3>增加文件打开数</h3>

<p>第一步最为基础的，提高打开文件描述符的数量。默认的情况下，这个配置为1024，是不能满足我们的使用的。增加到999999个：</p>

<p><code>
$ sudo ulimit -n 999999
</code></p>

<p>持久化这个配置，可以在<code>/etc/security/</code>（或<code>/etc/security.d/</code>，取决于你的发行版）下建立文件，增加</p>

<p><code>
*       hard    nofile      999999
*       soft    nofile      999999
</code></p>

<p>这将对所有用户起效。</p>

<h3>增加客户端端口数</h3>

<p>当Linux发起客户端连接时，如果没有显式指定，会给客户端socket绑定一个 ephemeral 端口。这个端口的范围是从这个区间选取的：</p>

<p>```
 $ cat /proc/sys/net/ipv4/ip_local_port_range
32768   61000</p>

<p>```</p>

<p>如果这个区间的端口耗尽，socket就会产生<code>cannot assign requested address</code>的错误。要增加端口范围，我们需要把他设置得更大：</p>

<p><code>
$ sudo echo "1025 65535" &gt; /proc/sys/net/ipv4/ip_local_port_range
</code></p>

<p>这样，单台机器就可以发出六万多个连接。</p>

<h3>增加虚拟网卡</h3>

<p>对于内存大一点的客户端机器，六万多个连接远不是其性能极限。由于IP消息中，一条消息是由 <code>src_addr</code>, <code>src_port</code>, <code>dst_addr</code>, <code>dst_port</code> 四元组标识，所以要增加连接，我们需要更多IP。在Linux上，我们可以启动虚拟网卡绑定额外的IP。</p>

<p><code>
$ sudo ifconfig eth0:0 192.168.1.100
$ sudo ifconfig eth0:1 192.168.1.101
...
</code></p>

<p>要关闭这些虚拟网卡</p>

<p><code>
$ sudo ifconfig eth0:0 down
</code></p>

<h3>使用虚拟网卡连接</h3>

<p>拥有多个IP之后，客户端socket需要显示绑定这些IP才行，以python为例，在connect前调用：</p>

<p><code>python
sock.bind((local_addr, local_port))
</code></p>

<p>可以指定连接的源地址和端口。在普通的Linux编程里，当你指定<code>local_port</code>为<code>0</code>时，Linux会分配一个之前提到的 ephemeral 端口。但是当使用虚拟IP时，如果仍然指定0，系统并不会因为IP不同而重用端口号，达到六万多的限制后，仍然会抛出不能获得地址的异常。</p>

<p>实际上是可以获得的，这里需要用户显式地指定端口好。如果需要大规模的连接，那就一个一个绑定好了。</p>

<h3>启用time_wait reuse和recycle</h3>

<p>Linux的socket进入<code>time_wait</code>后需要有一定的时间回收，之后端口才能重新使用。这在大规模测试的时候就比较麻烦，为了免去等待，可以打开<code>tw_reuse</code>和<code>tw_recycle</code>这两个选项。</p>

<p><code>
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_recycle
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_reuse
</code></p>

<p>注意这两个选项都比较激进，最好仅在测试机上使用。</p>
]]></content>
  </entry>
  
</feed>
