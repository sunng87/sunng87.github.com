<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Programming | Here comes the Sun]]></title>
  <link href="http://sunng87.github.io/blog//blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://sunng87.github.io/blog//"/>
  <updated>2014-08-19T16:56:10+08:00</updated>
  <id>http://sunng87.github.io/blog//</id>
  <author>
    <name><![CDATA[Sun Ning]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AVOS Cloud 实时通信服务架构：微服务和服务发现]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/08/19/avoscloud-rtm-architecture-microservice-and-service-discovery/"/>
    <updated>2014-08-19T16:46:20+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/08/19/avoscloud-rtm-architecture-microservice-and-service-discovery</id>
    <content type="html"><![CDATA[<p>“微服务” (Microservice) 是今年特别热的一个概念，<a href="http://martinfowler.com/articles/microservices.html">Martin Fowler 的文章</a>对微服务作了详细的介绍。简而言之，微服务鼓励用户把功能拆分以细粒度的服务接口暴露出来，并通过REST 服务或轻量级消息队列集成。在微服务架构里，一个业务的实现，可能由不同的功能单元组合而实现。</p>

<p>在 AVOS Cloud，我们提供数据存储、统计、实时通信等不同功能的服务，在实现上，这些功能需要共用基础设施，有的服务本身也根据业务性质的不同拆分功能模块，我们目前就是以这种“微服务”架构思路来实现拆分。有句话说，if you cannot split, you cannot scale.</p>

<p>以实时通信服务为例，根据功能角色的不同，我们有这样一些模块：</p>

<ul>
<li>Push 服务：处理推送的订阅关系，触发推送</li>
<li>长连接服务器：维持设备与服务的长连接</li>
<li>Router：应用层的 lookup，负责分配合适的长连接服务器给新设备</li>
<li>WatchDog：从多台长连接服务器收集运行和统计数据，对异常情况发起报警</li>
<li>数据存储：群组数据，用户可以通过API访问</li>
<li>HBase：消息记录存储</li>
</ul>


<p>模块间的集成，根据业务的特性分别使用 <a href="https://github.com/sunng87/slacker">Slacker</a> 远程调用框架和 <a href="http://twitter.github.io/kestrel/">Kestrel</a> 消息队列。
对于可能耗时较大的任务，我们通过 Kestrel 队列放到后台执行，避免阻塞前台服务，影响吞吐量。而另一些需要实时的集成，则使用 Slacker 远程调用实现多个进程间的通信。</p>

<p>Slacker 是一个专门为 Clojure 语言设计的远程调用框架，利用 Slacker 你可以暴露一个 或多个 Clojure 的 namespace 供远程调用。在客户端，Slacker 利用 <a href="http://clojure.org/macros">Clojure 宏</a>的特性，保持远程调用和本地调用的代码完全一致，这样本地和远程调用的切换只要更改一个 <code>(require)</code> 即可实现，把框架对业务代码的侵入降到最低。此外，Slacker 使用二进制序列化 <a href="https://github.com/ptaoussanis/nippy">nippy</a>，在网络连接层面使用异步复用，同时在超时方面也做了良好的控制。</p>

<p>以上的基础设施帮助我们良好地拆分模块，为下一步的扩展提供了可能。</p>

<h3>服务发现</h3>

<p>长连接服务器是实时通信的功能核心，它的瓶颈在内存和 CPU，可以通过增加部署来达到线性扩展。随着业务量的增加和硬件资源的整合，它可能会面临较频繁的部署变化，另外它也需要有能通过新增部署来快速平滑高峰压力的能力。基于 <a href="https://en.wikipedia.org/wiki/Publish/subscribe">Pub/Sub 抽象</a>的消息队列对此有良好的支持，但这对我们以 RPC 为核心的集成方式提出了新的要求，依赖模块也能快速响应服务部署的变化：我们不可能在新增某个服务部署后修改每个依赖的配置再逐一重启。</p>

<p>在这方面，我们利用了 <a href="https://github.com/sunng87/slacker-cluster">Slacker Cluster 框架</a>。他的核心思想是在部署和服务间增加一层抽象：对于服务的消费者而言，只需声明自己所依赖的服务，而无需静态地了解进程的地址。</p>

<p>所有的服务提供者将自己能够提供的服务注册在 <a href="https://zookeeper.apache.org/">Zookeeper</a> 集群里，并将部署地址注册为 Ephemeral 节点。Ephemeral 节点在创建它的连接断后会自动删除，这样当一个部署下线后，它相应的节点也会自动删除。</p>

<p>```bash</p>

<h1>Zookeeper 目录结构</h1>

<p>ls /slacker/example-cluster/namespaces/
[my.serviceA, my.serviceB]</p>

<p>ls /slacker/example-cluster/namespaces/my.serviceA
[192.168.1.100:2104, 192.168.1.101:2014&hellip;]
```</p>

<p>所有服务的客户端会 watch 自己感兴趣的 Zookeeper 节点，而部署变化时，所有的客户端都会得到通知，进而刷新服务列表，将流量引向新的节点。</p>

<p>在实时通信服务中，Router 服务会通过这个 RPC 机制轮询所有在线的长连接服务器，记录他们实时的运行状态。所有的用户设备并非直接连接到固定的长连接服务器，而是先询问 Router，由后者分配一台压力较轻的实例。当有新的长连接服务器部署后，Router收到通知，新的连接将优先连接这个新进程。此外，监控和数据收集的服务也会自动地把新实例加入管理范围。</p>

<p>有了这样一套服务发现机制，我们就可以对整个架构中的任意模块随时增减部署，保证服务可以以健康的状态运行。未来，我们还会集成云主机的提供商的API，来实现基础设施的自动化：当系统压力达到阀值时，云主机自动分配新的资源自动开机，jenkins 自动部署，加上现有的服务发现机制，实现0手工操作。这将是云服务运维的新篇。</p>

<p>原载 <a href="https://blog.avoscloud.com/1927/">AVOSCloud Blog</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在 Docker 中安装和使用 Rust Nightly 版本]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/08/02/rust-with-docker/"/>
    <updated>2014-08-02T18:02:12+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/08/02/rust-with-docker</id>
    <content type="html"><![CDATA[<p>一直关注 Rust 语言，最近一下发现了两个 web 框架，<a href="http://ironframework.org">Iron</a> 和 <a href="http://nickel.rs">Nickel.rs</a>。先不说这两个框架成熟度如何，一般情况下，一个语言有了 web 框架，算是一个里程碑，说明他离靠谱也不远了。这样我决定跟一下 nightly 版本（新框架都是跟 nightly），另外也能感受一下 <a href="https://mail.mozilla.org/pipermail/rust-dev/2014-March/009090.html">Yehuda Katz 的构建工具 Cargo</a>。ArchLinux 的仓库里已经有 0.11 版本，再用脚本安装必然会有冲突。于是想到了最近半年<a href="https://twitter.com/jessenoller/status/495037475421954048">最火的 Docker</a>，可以轻松的创建多个环境，正是一个非常好的场景。</p>

<h2>安装</h2>

<p>安装 Docker, Arch Linux 仓库里很早就有，非常方便：<code>sudo pacman -S docker</code>。完成之后启动他：<code>sudo systemctl start docker</code>。</p>

<p>之后我们拉一个 ubuntu 的镜像下来：<code>docker pull ubuntu</code>。</p>

<p>完成之后，我们启动一个 container，做一些基本的 setup：<code>docker run -i -t ubuntu:14.04 /bin/bash</code></p>

<p>这相当与运行在 ubuntu:14.04 这个镜像上运行一个 shell，接下来就进入了这个 shell 环境，和 ubuntu 安装版本完全一致，我们做一些基础的准备，安装一些必要的工具：<code>apt-get install build-essentials git curl libssl-dev</code></p>

<p>之后，就可以下载 Rust 提供的脚本来安装 nightly 版本了：<code>curl -s http://www.rust-lang.org/rustup.sh &gt; rustup</code></p>

<p>这里有个问题，rustup 脚本判断64位系统时会出错导致安装失败：
```sh</p>

<h1>Detect 64 bit linux systems with 32 bit userland and force 32 bit compilation</h1>

<p>if [ $CFG_OSTYPE = unknown-linux-gnu -a $CFG_CPUTYPE = x86_64 ]
then</p>

<pre><code>file -L "$SHELL" | grep -q "x86[_-]64"
if [ $? != 0 ]; then
    CFG_CPUTYPE=i686
fi
</code></pre>

<p>fi
```</p>

<p>因为在我的机器上已知系统是64位，就强行绕过了他的判断。</p>

<p>```bash
if [ $CFG_OSTYPE = unknown-linux-gnu -a $CFG_CPUTYPE = x86_64 ]
then</p>

<pre><code>file -L "$SHELL" | grep -q "x86[_-]64"
if [ $? == 0 ]; then
    CFG_CPUTYPE=i686
fi
</code></pre>

<p>fi
```</p>

<p>之后执行 rustup 就可以直接安装最近的 rustc 和 cargo 了。安装完成执行 <code>rustc -v</code> 和 <code>cargo --version</code> （两个工具还不统一！）可以了解安装情况。</p>

<p>exit 退出 shell，commit 你的镜像，这样一个干净的镜像要好好保存：<code>docker commit IMAGE_ID sunng/rust-nightly</code></p>

<h2>Hello World</h2>

<p>之后可以写点代码了，我们不在 docker 里写，我们在 host 机器上写，然后挂载到 docker 上，因此 emacs 什么的也不用配置了。</p>

<p>创建一个目录，比如在 <code>$HOME/var/docker/helloworld</code>下，最简单的 rust 项目只要两个文件： <code>Cargo.toml</code> 和 <code>src/main.rs</code>。</p>

<p>```</p>

<h1>Cargo.toml</h1>

<p>[package]</p>

<p>name = &ldquo;hello-world&rdquo;
version = &ldquo;0.1.0&rdquo;
authors = [ &ldquo;<a href="&#109;&#97;&#105;&#x6c;&#116;&#x6f;&#58;&#115;&#x75;&#110;&#x6e;&#103;&#x40;&#x61;&#x62;&#111;&#x75;&#116;&#x2e;&#109;&#101;">&#x73;&#x75;&#x6e;&#110;&#103;&#64;&#97;&#x62;&#x6f;&#117;&#x74;&#x2e;&#109;&#x65;</a>&rdquo; ]
```</p>

<p>```rust
//main.rs</p>

<p>fn main() {
  println!(&ldquo;hello world&rdquo;);
}</p>

<p>```</p>

<p>构建项目不需要手动 rustc 了，那是上个世纪的东西，我们直接 <code>cargo build</code> 就可以：<code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld sunng/nightly cargo build</code></p>

<p>其中 <code>-v</code> 参数用于挂载目录，<code>-w</code> 参数指定执行的 pwd。</p>

<p>如果构建成功，就可以执行了，在 docker 中执行：<code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld sunng/nightly target/hello-world</code></p>

<p>其实可以直接在 host 系统里执行也是完全可以的：<code>$HOME/var/docker/helloworld/target/hello-world</code>。</p>

<h2>Web Hello World</h2>

<p>前面说了 Rust 都有 web 框架了，我们就写一个 Web 版本的 Hello World 吧。这次用 Iron 框架，首先添加依赖到 Cargo 文件：</p>

<p>```
[package]</p>

<p>name = &ldquo;hello-world&rdquo;
version = &ldquo;0.1.0&rdquo;
authors = [ &ldquo;<a href="&#x6d;&#x61;&#x69;&#x6c;&#116;&#x6f;&#x3a;&#x73;&#117;&#110;&#110;&#103;&#x40;&#x61;&#x62;&#x6f;&#x75;&#116;&#x2e;&#109;&#101;">&#x73;&#117;&#110;&#110;&#x67;&#x40;&#97;&#98;&#x6f;&#x75;&#x74;&#x2e;&#109;&#x65;</a>&rdquo; ]</p>

<p>[dependencies.iron]</p>

<p>git = &ldquo;<a href="https://github.com/iron/iron.git">https://github.com/iron/iron.git</a>&rdquo;</p>

<p>[dependencies.core]</p>

<p>git = &ldquo;<a href="https://github.com/iron/core.git">https://github.com/iron/core.git</a>&rdquo;
```</p>

<p>Cargo 目前还没有中央仓库，但是<a href="http://crates.io/faq.html#github">据说将来会有</a>。目前还都是用 git 仓库来直接添加，所以构建环境里必须要有 git。</p>

<p>照着 <a href="https://github.com/iron/iron/blob/master/examples/hello.rs">Iron 的例子</a>写一个最简单的 hello world 程序。</p>

<p>```rust
extern crate iron;
extern crate http;</p>

<p>use std::io::net::ip::Ipv4Addr;
use iron::{Iron, Server, Chain, Request, Response, Alloy, Status, Unwind, FromFn};
use <a href="http::status;">http::status;</a></p>

<p>fn hello_world(<em>: &amp;mut Request, res: &amp;mut Response, </em>: &amp;mut Alloy) &ndash;> Status {</p>

<pre><code>res.serve(status::Ok, "Hello, world!");
Unwind
</code></pre>

<p>}</p>

<p>fn main() {
  let mut server: Server = Iron::new();
  server.chain.link(FromFn::new(hello_world));
  server.listen(Ipv4Addr(127, 0, 0, 1), 3000);
}</p>

<p>```</p>

<p>编译 <code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld sunng/nightly cargo build</code></p>

<p>运行 <code>docker run -i -t -v $HOME/var/docker:/mnt/data -w /mnt/data/helloworld -p 3000:3000 sunng/nightly target/hello-world</code></p>

<p>新增的参数<code>-p</code>是把 docker 环境里的端口3000映射到 host 上的3000，这样我们才能在外面访问。</p>

<p>最后还有一个问题，因为程序听的是127.0.0.1，所以在 host 上是无法访问这个端口的，修改代码：</p>

<p>```rust</p>

<p>  server.listen(Ipv4Addr(0, 0, 0, 0), 3000);
```</p>

<p>就可以正常工作了。</p>

<h2>Wrap up</h2>

<p>总结一下上面用 docker 比虚拟机的好处：</p>

<ul>
<li>占用资源少，启动快</li>
<li>与 host 共享网络、硬盘都非常方便，满足开发需要不成问题</li>
<li>所有都是命令，与 host 系统上的进程集成也非常方便</li>
<li>支持镜像的版本控制和仓库</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clojure Microservice Architecture With Slacker Cluster]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/07/08/microservice-and-slacker-cluster/"/>
    <updated>2014-07-08T22:12:52+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/07/08/microservice-and-slacker-cluster</id>
    <content type="html"><![CDATA[<p><a href="http://www.infoq.com/presentations/Micro-Services">Microservice</a> has been a hot new concept in these days. Martin Fowler explained microservice <a href="http://martinfowler.com/articles/microservices.html">in this article</a>. From me, microservice is a set of fine-grained function units running on independent process, each of them are connected with light-weighted transports: RESTful API or light messaging queue.</p>

<p>It&rsquo;s a new concept in enterprise architecture, since the last movement in the field promotes SOA architecture. SOA encourages architects to componentize their business logic in service, and deploy service bus(ESB) for integration. Microservice can be more concrete and light-weighted. The service units in Microservice can be any standalone function, or just a tier in traditional tier based development. These units can be deployed on dedicate process or grouped into a process.</p>

<p>In clojure development at <a href="https://avoscloud.com">avoscloud</a>, we are using the <a href="https://github.com/sunng87/slacker-cluster">slacker cluster framework</a> for our microsrvice architecture.</p>

<p><a href="https://github.com/sunng87/slacker">Slacker RPC</a> exposes services as  clojure namespace (pretty light-weighted) All functions in the namespace can be called from remote. A slacker server can expose any number of namespaces:</p>

<p><code>clojure
(start-slacker-server 2014 [my.serviceA my.serviceB ...])
</code></p>

<p>Slacker uses a binary protocol on TCP and configurable serialization (json/edn/<a href="https://github.com/ptaoussanis/nippy">nippy</a>) for communication, which is fast and compact.</p>

<p>And in slacker cluster, exposed namespaces are registered on zookeeper as ephemeral nodes. The client doesn&rsquo;t have to know which service is deployed on which process. Instead, it connects to zookeeper and look up all process address for service it interests in.</p>

<p>```clojure
(def sc (clustered-slacker-client zk-addr &hellip;))
(defn-remote &lsquo;sc my.serviceA/fn-abc)</p>

<p>;;when calling remote function for the first time, the client will look up zookeeper for remote processes and cache the results
(fn-abc)
```</p>

<p>If there are more than one process available, the client library will balance the load on each process. And for stateful service, slacker cluster also elects master node to ensure all requests go to single process. (<a href="http://sunng.info/blog/blog/2014/06/09/grouping-in-slacker-0-dot-12/">Slacker cluster grouping</a>)</p>

<p>Zookeeper directory structure:</p>

<p>```
ls /slacker/example-cluster/namespaces/
[my.serviceA, my.serviceB]</p>

<p>ls /slacker/example-cluster/namespaces/my.serviceA
[192.168.1.100:2104, 192.168.1.101:2014&hellip;]
```</p>

<p>Decoupling processes and services made microservice deployment quite flexible. Functional namespaces can be deployed on any process, standalone or grouped together, like Martin Fowler&rsquo;s chart <a href="http://martinfowler.com/articles/microservices/images/sketch.png">shows</a>.</p>

<p>All these nodes are also watched by clients. If a process crashed or put offline, the clients will get notified by zookeeper, then no requests will be made on that process. Also, when you exhausted service capacity, just simply put on another process, the client will soon balance load to the new node. Scaling services is easy like that.</p>

<p>Thanks to zookeeper&rsquo;s watch mechanism, there&rsquo;s no need to configure service static and update while you add/remove nodes. This is especially important in large-scale deployment. (Since microservices are often find-grained, you will always have a lot of process to update/restart.)</p>

<p>For more about Slacker Cluster, <a href="https://github.com/sunng87/slacker-cluster">check my code repository</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[扩展 Linux Ephemeral 端口限制]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/07/01/extend-linux-ephemeral-ports/"/>
    <updated>2014-07-01T17:11:58+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/07/01/extend-linux-ephemeral-ports</id>
    <content type="html"><![CDATA[<p>默认情况下，单一Linux能发起的客户端连接数是十分有限的，为此，我们要测试大规模连接程序时不得不启动很多客户端机器模拟连接。下面介绍一些增加单台Linux发起连接数的方法。</p>

<h3>增加文件打开数</h3>

<p>第一步最为基础的，提高打开文件描述符的数量。默认的情况下，这个配置为1024，是不能满足我们的使用的。增加到999999个：</p>

<p><code>
$ sudo ulimit -n 999999
</code></p>

<p>持久化这个配置，可以在<code>/etc/security/</code>（或<code>/etc/security.d/</code>，取决于你的发行版）下建立文件，增加</p>

<p><code>
*       hard    nofile      999999
*       soft    nofile      999999
</code></p>

<p>这将对所有用户起效。</p>

<h3>增加客户端端口数</h3>

<p>当Linux发起客户端连接时，如果没有显式指定，会给客户端socket绑定一个 ephemeral 端口。这个端口的范围是从这个区间选取的：</p>

<p>```
 $ cat /proc/sys/net/ipv4/ip_local_port_range
32768   61000</p>

<p>```</p>

<p>如果这个区间的端口耗尽，socket就会产生<code>cannot assign requested address</code>的错误。要增加端口范围，我们需要把他设置得更大：</p>

<p><code>
$ sudo echo "1025 65535" &gt; /proc/sys/net/ipv4/ip_local_port_range
</code></p>

<p>这样，单台机器就可以发出六万多个连接。</p>

<h3>增加虚拟网卡</h3>

<p>对于内存大一点的客户端机器，六万多个连接远不是其性能极限。由于IP消息中，一条消息是由 <code>src_addr</code>, <code>src_port</code>, <code>dst_addr</code>, <code>dst_port</code> 四元组标识，所以要增加连接，我们需要更多IP。在Linux上，我们可以启动虚拟网卡绑定额外的IP。</p>

<p><code>
$ sudo ifconfig eth0:0 192.168.1.100
$ sudo ifconfig eth0:1 192.168.1.101
...
</code></p>

<p>要关闭这些虚拟网卡</p>

<p><code>
$ sudo ifconfig eth0:0 down
</code></p>

<h3>使用虚拟网卡连接</h3>

<p>拥有多个IP之后，客户端socket需要显示绑定这些IP才行，以python为例，在connect前调用：</p>

<p><code>python
sock.bind((local_addr, local_port))
</code></p>

<p>可以指定连接的源地址和端口。在普通的Linux编程里，当你指定<code>local_port</code>为<code>0</code>时，Linux会分配一个之前提到的 ephemeral 端口。但是当使用虚拟IP时，如果仍然指定0，系统并不会因为IP不同而重用端口号，达到六万多的限制后，仍然会抛出不能获得地址的异常。</p>

<p>实际上是可以获得的，这里需要用户显式地指定端口好。如果需要大规模的连接，那就一个一个绑定好了。</p>

<h3>启用time_wait reuse和recycle</h3>

<p>Linux的socket进入<code>time_wait</code>后需要有一定的时间回收，之后端口才能重新使用。这在大规模测试的时候就比较麻烦，为了免去等待，可以打开<code>tw_reuse</code>和<code>tw_recycle</code>这两个选项。</p>

<p><code>
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_recycle
$ echo 1 &gt; /proc/sys/net/ipv4/tcp_tw_reuse
</code></p>

<p>注意这两个选项都比较激进，最好仅在测试机上使用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slacker Cluster 0.12: Grouping]]></title>
    <link href="http://sunng87.github.io/blog//blog/2014/06/09/grouping-in-slacker-0-dot-12/"/>
    <updated>2014-06-09T21:06:39+08:00</updated>
    <id>http://sunng87.github.io/blog//blog/2014/06/09/grouping-in-slacker-0-dot-12</id>
    <content type="html"><![CDATA[<h2>What are Slacker and Slacker Cluster</h2>

<p><a href="https://github.com/sunng87/slacker">Slacker</a> is my side project started in late 2011. The goal of Slacker project is to provide a high performance RPC system for clojure, with elegant API. Slacker doesn&rsquo;t ruin your code. Your remote invocation looks exactly same as local, from code. That means you can switch back and forth at same time.</p>

<p><a href="https://github.com/sunng87/slacker-cluster">Slacker Cluster</a> is a support module for running Slacker servers with multiple instances. Cluster enabled slacker server will publish all its served namespaces to Zookeeper cluster. The Cluster enabled client reads and watches these meta data. The most important feature of Slacker Cluster is you can add or remove servers without changing client configuration.</p>

<h2>Grouping in Slacker Cluster</h2>

<p>Started in 0.11, then enhanced in 0.12, Slacker Cluster now has flexible <strong>grouping</strong> choices for your scenario. In Slacker Cluster, <strong>grouping</strong> means which server(s) to call on a particular invocation.</p>

<p>There and four kinds of grouping for you: <code>:random</code>, <code>:leader</code>, <code>:all</code> and custom.</p>

<h3>:random</h3>

<p>By default, Slacker cluster clients use <code>:random</code> grouping: select a random server from server list. Random grouping works great for stateless services. It automatically balances load of each server.</p>

<h3>:leader</h3>

<p>Slacker servers selects leader for each namespace they expose. So at any time there will be one and only one leader node for every namespaces. The <code>:leader</code> grouping routes all invocations onto the leader node. This is required when your server has state, and you have to ensure the consistency and availability.</p>

<h3>:all</h3>

<p>As the name suggests, <code>:all</code> grouping routes invocations on every node at same time. In other words, it&rsquo;s broadcast. Note that this grouping might change your function return values. In <code>:random</code> and <code>:leader</code> mode, there&rsquo;s only one server called, just like local invocation. In <code>:all</code>, there&rsquo;s chances several servers are called and several values returned. I will talk about how to deal with these return values later.</p>

<h3>Custom</h3>

<p>You can also provide a function for dynamic grouping. For requested namespace, function and arguments, you can specify any server(s) or grouping option.</p>

<h2>Grouping results</h2>

<p>Grouping may break original behavior of you code by returning multiple values from multiple servers. But you still have full control over it. There are four types of value you can specify for results aggregation: <code>:single</code>, <code>:vector</code>, <code>:map</code> and custom function.</p>

<p>In short words:</p>

<ul>
<li><code>:single</code> returns the first valid result, and behavior same as calling single server or local invocation. This is the default value.</li>
<li><code>:vector</code> returns  a vector of all results.</li>
<li><code>:map</code> returns a map of all results, indexed by server addresses.</li>
<li>Custom aggregation function accepts the results and allows you to merge the values.</li>
</ul>


<h2>Grouping exceptions</h2>

<p>What happens when remote function threw exceptions? The grouping exception option defines that. When set to <code>:all</code>, the client will raise an error only if all remote nodes broken. Otherwise, the broken result will be ignored and only valid results will apply <code>grouping-results</code> rules. The opposite option is <code>:any</code>, which mean client will raise error when any of calls is broken.</p>

<h2>Granularity</h2>

<p>The grouping options can be set to client level:</p>

<p><code>clojure
(clustered-slackerc "cluster-name" "127.0.0.1:2181" :grouping :leader)
</code></p>

<p>or function level:</p>

<p><code>clojure
(defn-remote sc slacker.example.api/timestamp
  :grouping :all
  :grouping-results :single)
</code></p>

<h2>Conclusion</h2>

<p><a href="https://github.com/sunng87/slacker-cluster">Slacker Cluster</a> has been used in our Avos Cloud backend for service integration. Feel free to let me know if you have interests or questions with this library.</p>
]]></content>
  </entry>
  
</feed>
